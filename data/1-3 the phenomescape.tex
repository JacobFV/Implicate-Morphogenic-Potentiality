\section{The Phenomenascape}

The nature of subjective experience is characteristically idiosyncratic because it consists of the building blocks specific to the mind in question. For this reason, two minds, appearing completely identical, but sharing a single difference in the perception or relations associated with a particular symbol, will yield exponentially greater differences in their interpretations of ideas as time progresses unless a corrective signal from either one or a third source entrains both. Formally, we would say a conceptual construction $c$ becomes increasingly differentiated as more information is incorporated into its definition $H(c_a) < H(c_b), a < b$. Although there may be intersections between various conceptual constructions, with the intersections representing points of mutual accordance between the conceptual frameworks, the symbols and steps necessary to traverse the space of ideas in order to reach those points are entirely unique, like fingerprints. These fingerprints are shaped not only by the biological substrates facilitating the considerations being made, but also by the individual histories of learning which shaped the resolution of the ideas. Without some force imposing similarity of development or execution upon the minds under consideration, it is as impossible for their trains of thought to be equivalent as it is for two minds to be exact replicas in form, or for two individuals to share the same life experience.

The acquisition of these constituent ideas or symbols take place via the development of the neural mechanisms that allow recognition to take place, as well as the concepts that we learn to apply. For this reason, a human-being that has never learned to recognize written language will be limited in their ability to formulate ideas that would generally require the synthesis of knowledge gained through self-study. Likewise, an organism or mind lacking the cognitive architecture necessary to make sense of -- or distinguish between the various gradations of -- a concept cannot be expected to systematically perform cognitive work upon these concepts beyond what be expected by pure chance. In this way, we can consider the processing of ideas to be analogous to the transformations of matrices containing relevant data inputs, giving rise to increasingly complex, and hopefully, useful, new ideas. By performing these transformations, one is applying a concept to other concepts or contexts, and making use of their emergent structure. This is why it is unfair to expect a baby or a dog to drive a car but why we assume a developed human being of average ability should be capable of learning how to. Similarly, it is expected that schoolchildren following a rational trajectory of learning to recognize the quantitative units manipulated via arithmetic in increasingly abstract processes, beginning with something akin to algebra, passing through geometry, trigonometry, and eventually, calculus. The reason this progression takes place in the normally expected course of learning, and not in some other order, roughly speaking, is that it is necessary to lay the foundation for more complex ideas with simpler ideas that allow the mind to navigate the problem space. Using simpler ideas, the mind is able to approach more complex ones -- and ultimately reduce them! -- in terms of existing understanding, until it becomes a fundamental element of understanding itself. In other words, by coming to grips with a concept in ways we can already articulate, the new concept eventually becomes second nature, and becomes a new pathway through which we can understand future ideas.

This approach towards equivalence composed of preexisting concepts is itself equivalent to compression in some higher-order space where the conceptual trajectory towards some end representation becomes another representation, symbol, or dimension in that very space! Consider an example in vector space:

Let's start with an example: memory. When the human brain performs active recall on a given topic, it actually undergoes a process of reconstruction via association. For this reason, every recollection is slightly different, but each one is usually anchored on the ideas that provide the most information about it. By prioritizing concepts based on their relevance to the end goal, and not in terms of their physical explanations, we increase the ease with which we process concepts. So when I remember that I need to buy a gallon of milk at the grocery store, it is mostly by first remembering that I was planning to prepare a particular meal, and then that milk is required for that meal, and then that milk is indeed associated with this grocery store, maybe even with specificity extending to the exact aisle within which it can be found. In this way, I remember the milk in terms of relations with multiple entities (dinner, store, meal prep, maybe aisle) and not in terms of exactly what the milk at its particular location is.

Another example: we assign names to things and each other so that we can retrieve relevant information about them from the embedding space representing their categories, which are broader and less defined. And then, once the idea is acquired and understood well enough to be recognized and repeated, we can consciously extend its embedding space via analogy. For this reason, language could be considered the ultimate embedding space, where definitions can undergo arbitrarily complex refinement, and then be compared to all other and future definitions entered into the vocabulary.

Now the formalization:

Show symbolic example

Show how symbolic form can be equivalent to vector form under certain conditions

(bring up some point about parallel composition, integration, or differentiation) So while we have compared the trajectory of experience to an infinitesimal point traversing its representation space, we more aptly consider this entity possessing spatial form.

The more information this entity consumes, the larger its frontal area, and therefore, increasing entropy flux.

However as

Event horizon of expansion

Let us state this directly, riding on the scientific frameworks built over the last several hundred years, we are describing minds as abstract geometric entities flying through representation space and collecting some of the particulates of information they collide with along the way. Astrophysics, aerodynamics, gravity, relativity, curvature, black holes -- we carry their essences right along into this framework. Just a geometric analogies help us to "see" the structure of representations, transformations, etc., these higher order abstractions give us a more ameable -- and should any of these fields ultimately prove strongly emergent from their underlying substrate -- the only framework for approaching the immeasurable complexity of intelligence.

To explain what we mean, let us start with Math: not the synthetic kind finite and fully described with physical symbol manipulations, but the abstract sort, intrinsic to the latent causal structures of the universe, and born within its pockets of regularity. The last point is by necessity, for without regularity, no structure can exist.  Even the fundamental laws of science must ultimately rest on Math. It is this intangible Math which intelligence makes inferences on by interacting with the data it generates.

We have already discussed at length how mathematics gives rise to geometric structure. And so here we continue that thread by consider the physics of these increasingly complex structures: at large scale, networks of implication strongly emerge into differentiable geometric forms. These


While Math itself does not change -- either a property exists or it does not -- the local perception of it may be. Truth testing, for instance, requires establishing some reference statement deemed as truth to compare against.

TODO: climb the ladder of abstraction until i get to the morphic form of the mind with its differentiated perceptive geometry. nono thats too ocmplex. just stick with relativistic physics